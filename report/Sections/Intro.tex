\clearpage
\chapter{Introduction}

Nowadays, with the advent of modern computing, there has been a huge push towards taking advantage of these resources. In the scientific world, there is a constant drive towards building optimised libraries for performing operations, spanning areas such as, basic linear algebra~\cite{blas}, fast Fourier transformations~\cite{fftw} or genetic algorithms for machine learning~\cite{galib}. All of these libraries have been written with the intentions of exhausting as much processing power, memory and parallelisation as possible. These libraries are available across all kinds of architectures, Intel's MKL for example built for Intel CPUs, Nvidia's CUDA SDK written for their own GPUs or MAGMA, a 3\textsuperscript{rd} party library written for heterogeneous architectures. These advancements in scientific computing have not come from nowhere, but rather are becoming duly necessary as most modern problems in science become impossible to solve without taking advantage of modern computing resources. A clear example of this was seen last year when the first image of a black hole was rendered, using a novel image cleaning machine learning algorithm and 5 petabytes of data - clearly something that cannot be done without some form of distributed memory architecture.

Nvidia have been among the leaders of this charge from both a hardware and software point of view. Previously, GPUs were only used for their actual purpose, for performing graphics manipulation for games and rendering. However, since around 2001, these cards have effectively been hijacked to perform all kinds of operations since they became stacked with programmable floating point operations. This general-purpose GPU programming, or GPGPU, has opened up a world of opportunity to take advantage of the hardware available and perform scientific operations. Nvidia now even offer cards which do not even provide any ports for graphical output. Nvidia have taken to developing their own set of mathematical libraries to be used with their cards and developed the CUDA language to implement this. Not only this, but in the last few editions of their architectures, since Fermi and Turing, they have began to add extra accelerator chips on to their cards for performing fast tensor  operations and Ray Tracing for machine learning.

The need for taking advantage of parallelism in modern science now clear, one problem that crops up in a vast array of areas is solving partial differential equations, or PDEs. These are relationships between variables and their partial derivatives and can be seen in areas such as fluid dynamics with the Navier-Stokes equations, in finance with the Black-Scholes equation or even in engineering when modelling stress of a structure - an important one for the topic at hand in this paper. Unfortunately, while analytic solutions exist for theoretical problems of this nature, studied in undergraduate courses, in the real world most of these PDEs are not of this convenient solvable nature and thus require numerical methods to solve. There are many variants of numerical methods which one can use to solve PDEs, some more simple than others, such as the finite difference method which decomposes the domain into a simple grid and approximates the derivatives, and some more complex but robust, like meshfree methods which create a collection of Voronoi cells on the domain instead of a basic grid - allowing for more complex structures~\cite{mfree}. Certain methods land somewhere in a middle ground of both complexity and adaptability such as the finite volume method and the finite element method - the later of which will form the basis of this study.

The finite element method~\cite{strang} is a numerical method developed originally for use in engineering for modelling stress on structures and has since quickly expanded to use in all branches of science such as electrostatics, stress analysis for crash simulations and modelling heat transfer. Its engineering foundations will become clear throughout the paper as much of the terminology has remained unchanged. Just as there are libraries for linear algebra and other mathematical methods mentioned previously, certain libraries for applying the FEM already exist, such as FEniCS~\cite{fenics}, FreeFEM~\cite{freefem}, and even Wolfram Alpha have their own applications inbuilt to their robust Mathematica software~\cite{walpha}. Most of these implementations are all written for serial or CPU-based architectures.

This paper investigates current approaches to applying the finite element method on Nvidia GPUs and attempts at isolating and pinpointing certain bottlenecks for parallelism upon which may be improved. The importance of this is clear, as PDE-related problems get larger and more complex, the  need for more computing power is evident to get approximate solutions in reasonable amounts of time.